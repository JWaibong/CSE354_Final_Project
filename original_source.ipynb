{"cells":[{"cell_type":"markdown","metadata":{"id":"KWMGPce-Cu_J"},"source":["# **CSE354 HW3**\n","**Due date: 11:59 pm EST on April 21, 2023 (Friday)**\n","\n","---\n","For this assignment, we will use Google Colab, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You can use your Stony Brook (*.stonybrook.edu) account or your personal gmail account for coding and Google Drive to save your results.\n","\n","## **Google Colab Tutorial**\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","**This notebook would need you to train your model on Colab's GPU. However, the runtimes are limited. So ensure that your code works on the default CPU runtime before switching over to the GPU runtime.**\n","\n","## **Problem statement**\n","---\n","In this homework, you will be using language models to predict the sentiment of a given movie review. The dataset, which is given to you, is sampled from the [IMDB dataset of 50k movie reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). The sentences are sampled to a smaller set to help with quicker computation on Colab. The data contains a review and an associated label for the sentiment of that review. The label can either be *positive* or *negative*. You have been given three files - train_data.csv, val_data.csv and test_data.csv. The training data will be used to fine-tune the language model, the val data will be used to select the best model while training and finally the test data will give the model's final performance on the data.\n","\n","To perform this task you will be using a pre-trained DistilBERT model. DistilBERT is a BERT based language model. Its size is 40% lesser than BERT, it has around 97% of BERT's language understanding capabilities and is 60% faster. You can read more about DistilBERT - https://arxiv.org/abs/1910.01108.\n","\n","You will be using the model by taking advantage of the libraries provided by Hugging Face (https://huggingface.co/). In order to use this library, it will need to be installed using the command in the cell below. You will be training four different DistilBERT models for this assignment.\n","\n","**Todos for the assignment:**\n","*   Fill in the # TODO(students) portions in this Colab file for this assignment.\n","*   Run the experiment code blocks and note down the colab outputs in a separate text file.\n","*   Use the aformentioned colab outputs for writing the report as per submission guideline that is described at the end of this colab file.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"om1HnMs-Gw3c"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"I6-oi7JhGjjB"},"source":["## **Imports**\n","---\n","\n","All the allowed imports have been done for you in the code block below. You do need and will not be allowed to use any more imports other than the ones done below."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Q0N6kcCYG3tn"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2023-04-16 01:18:23.569756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-16 01:18:23.774908: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-04-16 01:18:23.826122: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-04-16 01:18:24.614425: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-04-16 01:18:24.614582: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-04-16 01:18:24.614589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import AdamW\n","import os\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","torch.manual_seed(42)\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"i8rQ5vERGwes"},"source":["## **Mounting your drive**\n","---\n","\n","I would highly recommend mounting you Google Drive while running this notebook. This drive could contain the path to your dataset and it will also be used to save your fine-tuned models. In case you choose to simply save the models on your Colab workspace, the models will cease to exist after the runtime disconnects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIrvZGP08z-1"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFyqw_sxnEh8"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xVvQyFjTm4hW"},"outputs":[],"source":["#Set the path of the folder where your colab file and data exist in Google Drive in the ------ porition\n","\n","# TODO(students): start\n","%cd \"drive/MyDrive/--------\"\n","# TODO(students): end"]},{"cell_type":"markdown","metadata":{"id":"kihyF-BLHgou"},"source":["## **Constants in the file**\n","---\n","\n","The code block below contains a few constants.\n","\n","\n","1.   **BATCH_SIZE**: The batch size input to the models. This has been set to 16 and should not be changed. In case you encounter any CUDA - out of memory errors while training your models, this value may be reduced from 16. But please mention this in your submission report.\n","2.   **EPOCHS**: The number of epochs to train your model. This should not be changed.\n","3. **TEST_PATH**: This is the path to the test_data.csv file.\n","4. **TRAIN_PATH**: This is the path to the train_data.csv file.\n","5. **VAL_PATH**: This is the path to the val_data.csv file.\n","6. **SAVE_PATH**: This is the path to directory your model will be saved. Note: This path will be altered further down in the code by appending the name of the kind of DistilBERT model you train as per your experiments.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XrivEEHR0kUq"},"outputs":[],"source":["#DO NOT CHANGE THE CONSTANTS\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","TEST_PATH = \"data/test_data.csv\"\n","TRAIN_PATH = \"data/train_data.csv\"\n","VAL_PATH = \"data/val_data.csv\"\n","SAVE_PATH = \"models/DistilBERT\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_iuId22dbbSq"},"outputs":[],"source":["def load_dataset(path):\n","  dataset = pd.read_csv(path)\n","  return dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"OQk4i6bH-LUo"},"outputs":[],"source":["train_data = load_dataset(TRAIN_PATH)\n","val_data = load_dataset(VAL_PATH)\n","test_data = load_dataset(TEST_PATH)"]},{"cell_type":"markdown","metadata":{"id":"Kz5glgyTJRkL"},"source":["## **Problem 1 (Initialize the Model Class)**\n","---\n","\n","Here, we will setup the pre-trained DistillBert model class in order to do our binary sentiment analysis task. In the code block below, you would need to load a pre-trained DistilBERT model and it's tokenizer using Hugging Face's library. The model you would need to load is called \"distilbert-base-uncased\". It would also need to have the model hyperparameter set to *num_classes* as the output shape of the model (in this case it is going to be 2, positive and negative). Please write your code between the given TODO block.\n","\n","\n","\n","More about the model and how to load it can be read at - https://huggingface.co/distilbert-base-uncased."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hmXIi5Y8fmcn"},"outputs":[],"source":["class DistillBERT():\n","\n","  def __init__(self, model_name='distilbert-base-uncased', num_classes=2):\n","    # TODO(students): start\n","    self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n","    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    # TODO(students): end\n","\n","  def get_tokenizer_and_model(self):\n","    return self.model, self.tokenizer\n","  \n","#x = DistillBERT()\n","# i=0\n","# for name, layer in x.model.named_parameters():\n","#   print(f\"{i}:{name}\")\n","#   i+=1\n","\n","# for y in x.model.modules():\n","#   print(f\"{i}:{y}\")\n","#   i+=1\n"]},{"cell_type":"markdown","metadata":{"id":"RZWzKZbiKMFP"},"source":["## **Problem 2 (Initialize the Dataloader Class)**\n","---\n","Here, we will setup the dataloader class which will read data, tokenize it using the DistillBert tokenizer, converts the tokenized sentence to tensors and the labels to tensors. The code block below takes your dataset(train,validation or test) and the tokenizer you loaded in the previous block and generates the DataLoader object for it. You would need to implement a part of the tokenize_data method. This method takes the given data and generates a list of token IDs for a given review along with it's label. You would need to use the tokenizer to generated the token ids (hint:refer to tokenizer.encode_plus for more details) values for each review. **Please ensure that the maximum length of an encoded review is 512 tokens. If any input data is longer than 512 words/tokens, truncate it to first 512.** \n","\n","You would also need to convert the labels to a corresponding numerical class using the label_dict dictionary. Please write your code between the given TODO block."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7yJHPbHud-nH"},"outputs":[],"source":["class DatasetLoader(Dataset):\n","\n","  def __init__(self, data, tokenizer):\n","    self.data = data\n","    self.tokenizer = tokenizer\n","\n","  def tokenize_data(self):\n","    print(\"Processing data..\")\n","    tokens = []\n","    labels = []\n","    label_dict = {'positive': 1, 'negative': 0}\n","\n","    review_list = self.data['review'].to_list()\n","    label_list = self.data['sentiment'].to_list()\n","\n","    for (review, label) in tqdm(zip(review_list, label_list), total=len(review_list)):\n","      # TODO(students): start\n","      encoding = self.tokenizer.encode_plus(review, max_length=512, truncation=True, pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')['input_ids'].to('cuda')\n","      tokens.append(encoding)\n","      labels.append(label_dict[label])\n","      # TODO(students): end\n","    \n","    tokens = pad_sequence(tokens, batch_first=True)\n","    labels = torch.tensor(labels)\n","    dataset = TensorDataset(tokens, labels)\n","    return dataset\n","\n","  def get_data_loaders(self, batch_size=32, shuffle=True):\n","    processed_dataset = self.tokenize_data()\n","\n","    data_loader = DataLoader(\n","        processed_dataset,\n","        shuffle=shuffle,\n","        batch_size=batch_size\n","    )\n","\n","    return data_loader"]},{"cell_type":"markdown","metadata":{"id":"VIF3eaP3Lh0W"},"source":["## **Problem 3 (Training Function)**\n","---\n","In this problem, you will write the code that will be used to run your model class on the dataset class, both of which you have written in the previous problems.\n","\n","The class below provides method to train a given model. It takes a dictionary with the following parameters:\n","\n","\n","1.   device: The device to run the model on.\n","2.   train_data: The train_data dataframe.\n","3.   val_data: The val_data dataframe.\n","4.   batch_size: The batch_size which is input to the model.\n","5.   epochs: The number of epochs to train the model.\n","6.   training_type: The type of training that your model will be undergoing. This can take four values - 'frozen_embeddings', 'top_2_training', 'top_4_training' and 'all_training'.\n","\n","#### **Problem 3(a)**\n","\n","Your first problem here would be to implement the set_training_parameters() method. In this method you will select the layers of your model to train based on the training_type. **Note: By default the Hugging Face DistilBERT has 6 layers.**\n","\n","1. frozen_embeddings: This setting is supposed to train the DistilBERT model with embeddings that are 'frozen' i.e., not trainable. You would need to ensure that the embedding layers in your model are not trainable.\n","2. top_2_training: This setting is supposed to train just the final two layers of DistilBERT (layer 5 and layer 4). All other layers before these would need to be frozen.\n","3. top_4_training: This setting is supposed to train just the final four layers of DistilBERT (layer 5, layer 4, layer 3 and layer 2). All other layers before these would need to be frozen.\n","4. all_training: All layers of DistilBERT would need to trained.\n","\n","Please write your code between the given TODO block.\n","\n","**Note: The classifier head on top of the final DistilBERT layer would always need to be trained, please do not freeze that layer.**\n","\n","**Note: You can use model.named_parameters() and iterate over all the named parameters of the model. To set the layers to not be trainable, apply layer.requires_grad = false**\n","\n","#### **Problem 3(b)**\n","\n","Your second problem would be to implement a single training step in the given loop inside the train() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would also need to propagate the loss backwards to the model and update the given optimizer's parameters.\n","\n","Please write your code between the given TODO block.\n","\n","#### **Problem 3(c)**\n","\n","Your second problem would be to implement a single validation step in the given loop inside the eval() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n","\n","Please write your code between the given TODO block.\n","\n","**Note: Consult the pytorch demos by the TAs during class for Problem 3(b) and 3(c).** (https://colab.research.google.com/drive/1Nf_5z4_g09KqOy0km4fyG4Kj2bRcEcCK?usp=sharing) "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"llVpLV9kiUST"},"outputs":[],"source":["class Trainer():\n","\n","  def __init__(self, options):\n","    self.device = options['device']\n","    self.train_data = options['train_data']\n","    self.val_data = options['val_data']\n","    self.batch_size = options['batch_size']\n","    self.epochs = options['epochs']\n","    self.save_path = options['save_path']\n","    self.training_type = options['training_type']\n","    transformer = DistillBERT()\n","    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n","    self.model.to(self.device)\n","\n","  def get_performance_metrics(self, preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n","    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n","    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n","    return precision, recall, f1\n","\n","  def set_training_parameters(self):\n","    # TODO(students): start\n","    t = self.training_type\n","    if t == 'frozen_embeddings':\n","      # will not turn on require_grad = True for any layer\n","      for name, layer in self.model.named_parameters():\n","        if 'classifier' in name:\n","          continue\n","        layer.require_grad = False\n","    elif t == 'top_2_training':\n","      # require_grad = True for layers 4,5\n","      for name, layer in self.model.named_parameters():\n","        if 'classifier' in name:\n","          continue\n","        if 'layer.4' in name or 'layer.5' in name:\n","          layer.require_grad = True\n","        else:\n","          layer.require_grad = False  \n","    elif t == 'top_4_training':\n","      #require_grad = True for layers 2,3,4,5\n","      for name, layer in self.model.named_parameters():\n","        if 'classifier' in name:\n","          continue\n","        if 'layer.2' in name or 'layer.3' in name or 'layer.4' in name or 'layer.5' in name:\n","          layer.require_grad = True\n","        else:\n","          layer.require_grad = False \n","    elif t == 'all_training':\n","      # require_grad = True for layers 0,1,2,3,4,5\n","      for name, layer in self.model.named_parameters():\n","        layer.require_grad = True\n","    else:\n","      raise KeyError(f\"training_type={t} not found\")\n","    # TODO(students): end\n","\n","  def train(self, data_loader, optimizer):\n","    self.model.train()\n","    total_recall = 0\n","    total_precision = 0\n","    total_f1 = 0\n","    total_loss = 0\n","\n","    for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n","      self.model.zero_grad()\n","      # TODO(students): start\n","      logits = self.model(reviews.squeeze(1)).logits\n","      loss = torch.nn.CrossEntropyLoss()(logits.to('cpu'), labels.to('cpu'))\n","\n","      loss.backward()\n","      optimizer.step()\n","      total_loss += loss\n","      precision, recall, f1 = self.get_performance_metrics(preds=logits.detach().cpu().numpy(), labels=labels.detach().cpu().numpy())\n","\n","      total_recall += recall\n","      total_precision += precision\n","      total_f1 += f1\n","      # TODO(students): end\n","\n","    precision = total_precision/len(data_loader)\n","    recall = total_recall/len(data_loader)\n","    f1 = total_f1/len(data_loader)\n","    loss = total_loss/len(data_loader)\n","\n","    return precision, recall, f1, loss\n","\n","  def eval(self, data_loader):\n","    self.model.eval()\n","    total_recall = 0\n","    total_precision = 0\n","    total_f1 = 0\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","      for (reviews, labels) in tqdm(data_loader):\n","        # TODO(students): start\n","        # self.set_training_parameters() not sure if necessary?\n","        logits = self.model(reviews.squeeze(1)).logits\n","        loss = torch.nn.CrossEntropyLoss()(logits.to('cpu'), labels.to('cpu'))\n","        \n","        total_loss += loss\n","        precision, recall, f1 = self.get_performance_metrics(preds=logits.detach().cpu().numpy(), labels=labels.detach().cpu().numpy())\n","\n","        total_recall += recall\n","        total_precision += precision\n","        total_f1 += f1\n","        # TODO(students): end\n","    \n","    precision = total_precision/len(data_loader)\n","    recall = total_recall/len(data_loader)\n","    f1 = total_f1/len(data_loader)\n","    loss = total_loss/len(data_loader)\n","\n","    return precision, recall, f1, loss\n","\n","  def save_transformer(self):\n","    self.model.save_pretrained(self.save_path)\n","    self.tokenizer.save_pretrained(self.save_path)\n","\n","  def execute(self):\n","    last_best = 0\n","    train_dataset = DatasetLoader(self.train_data, self.tokenizer)\n","    train_data_loader = train_dataset.get_data_loaders(self.batch_size)\n","    val_dataset = DatasetLoader(self.val_data, self.tokenizer)\n","    val_data_loader = val_dataset.get_data_loaders(self.batch_size)\n","    optimizer = torch.optim.AdamW(self.model.parameters(), lr = 3e-5, eps = 1e-8)\n","    self.set_training_parameters()\n","    for epoch_i in range(0, self.epochs):\n","      train_precision, train_recall, train_f1, train_loss = self.train(train_data_loader, optimizer)\n","      print(f'Epoch {epoch_i + 1}: train_loss: {train_loss:.4f} train_precision: {train_precision:.4f} train_recall: {train_recall:.4f} train_f1: {train_f1:.4f}')\n","      val_precision, val_recall, val_f1, val_loss = self.eval(val_data_loader)\n","      print(f'Epoch {epoch_i + 1}: val_loss: {val_loss:.4f} val_precision: {val_precision:.4f} val_recall: {val_recall:.4f} val_f1: {val_f1:.4f}')\n","\n","      if val_f1 > last_best:\n","        print(\"Saving model..\")\n","        self.save_transformer()\n","        last_best = val_f1\n","        print(\"Model saved.\")"]},{"cell_type":"markdown","metadata":{"id":"Bebt5gVsPSUi"},"source":["**Notes: Run the following blocks in order to train the model and save it in your Google Drive. There will be variations due to random initializations. Most of the experiment validation and test accuracy should be between 80%-95%. Each experiment should not take more than 30 minutes to run when runtime is set to GPU.**"]},{"cell_type":"markdown","metadata":{"id":"yt8LCF-CQRzS"},"source":["#### **Experiment 1**\n","---\n","Training your DistilBERT with frozen embeddings.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nWi6MQDJn3MO"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/5130 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 5130/5130 [00:03<00:00, 1298.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 270/270 [00:00<00:00, 1279.91it/s]\n","100%|██████████| 321/321 [01:50<00:00,  2.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: train_loss: 0.4494 train_precision: 0.7516 train_recall: 0.7502 train_f1: 0.7258\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:01<00:00,  8.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: val_loss: 0.2209 val_precision: 0.8983 val_recall: 0.9293 val_f1: 0.9032\n","Saving model..\n","Model saved.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [01:53<00:00,  2.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: train_loss: 0.1945 train_precision: 0.9321 train_recall: 0.9281 train_f1: 0.9249\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:01<00:00,  8.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: val_loss: 0.2547 val_precision: 0.9250 val_recall: 0.8543 val_f1: 0.8836\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [02:57<00:00,  1.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: train_loss: 0.0933 train_precision: 0.9701 train_recall: 0.9689 train_f1: 0.9667\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:05<00:00,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: val_loss: 0.2651 val_precision: 0.8776 val_recall: 0.9302 val_f1: 0.8971\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_frozen_embeddings'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'frozen_embeddings'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"cell_type":"markdown","metadata":{"id":"mZ_PDuUAQcGb"},"source":["#### **Experiment 2**\n","---\n","Training your DistilBERT with only top 2 layers being trained. \n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vrZS-fPSaq7g"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/5130 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 5130/5130 [00:05<00:00, 894.07it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 270/270 [00:00<00:00, 710.27it/s]\n","100%|██████████| 321/321 [04:39<00:00,  1.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: train_loss: 0.4269 train_precision: 0.7788 train_recall: 0.8032 train_f1: 0.7633\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:05<00:00,  3.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: val_loss: 0.2894 val_precision: 0.8243 val_recall: 0.9727 val_f1: 0.8860\n","Saving model..\n","Model saved.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [03:34<00:00,  1.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: train_loss: 0.2051 train_precision: 0.9217 train_recall: 0.9278 train_f1: 0.9184\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: val_loss: 0.2053 val_precision: 0.9058 val_recall: 0.9599 val_f1: 0.9274\n","Saving model..\n","Model saved.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [02:26<00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: train_loss: 0.0921 train_precision: 0.9664 train_recall: 0.9732 train_f1: 0.9674\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: val_loss: 0.2592 val_precision: 0.9119 val_recall: 0.9288 val_f1: 0.9183\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_top_2_training'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'top_2_training'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"cell_type":"markdown","metadata":{"id":"DLlQtrpOQhr1"},"source":["#### **Experiment 3**\n","---\n","Training your DistilBERT with only top 4 layers being trained. \n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"TMkzOXZja66z"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/5130 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 5130/5130 [00:04<00:00, 1108.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 270/270 [00:00<00:00, 1147.21it/s]\n","100%|██████████| 321/321 [02:31<00:00,  2.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: train_loss: 0.3749 train_precision: 0.8243 train_recall: 0.8186 train_f1: 0.7989\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: val_loss: 0.1812 val_precision: 0.9317 val_recall: 0.9131 val_f1: 0.9159\n","Saving model..\n","Model saved.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [02:35<00:00,  2.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: train_loss: 0.1739 train_precision: 0.9325 train_recall: 0.9389 train_f1: 0.9296\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: val_loss: 0.2185 val_precision: 0.9126 val_recall: 0.9254 val_f1: 0.9164\n","Saving model..\n","Model saved.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [02:34<00:00,  2.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: train_loss: 0.0790 train_precision: 0.9723 train_recall: 0.9743 train_f1: 0.9712\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.40it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: val_loss: 0.2665 val_precision: 0.9160 val_recall: 0.8934 val_f1: 0.8997\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_top_4_training'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'top_4_training'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"cell_type":"markdown","metadata":{"id":"bMqn3uoTQkwV"},"source":["#### **Experiment 4**\n","---\n","Training your DistilBERT with all layers being trained. \n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yd1DUHuZa9Zz"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/5130 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 5130/5130 [00:04<00:00, 1220.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 270/270 [00:00<00:00, 1191.19it/s]\n","100%|██████████| 321/321 [02:34<00:00,  2.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: train_loss: 0.3470 train_precision: 0.8551 train_recall: 0.8579 train_f1: 0.8381\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: val_loss: 0.2284 val_precision: 0.8750 val_recall: 0.9712 val_f1: 0.9165\n","Saving model..\n","Model saved.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [02:33<00:00,  2.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: train_loss: 0.1785 train_precision: 0.9364 train_recall: 0.9291 train_f1: 0.9270\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: val_loss: 0.2252 val_precision: 0.8910 val_recall: 0.9383 val_f1: 0.9073\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 321/321 [02:35<00:00,  2.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: train_loss: 0.0721 train_precision: 0.9793 train_recall: 0.9741 train_f1: 0.9750\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:02<00:00,  6.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: val_loss: 0.2594 val_precision: 0.9033 val_recall: 0.9655 val_f1: 0.9270\n","Saving model..\n","Model saved.\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['train_data'] = train_data\n","options['val_data'] = val_data\n","options['save_path'] = SAVE_PATH + '_all_training'\n","options['epochs'] = EPOCHS\n","options['training_type'] = 'all_training'\n","trainer = Trainer(options)\n","trainer.execute()"]},{"cell_type":"markdown","metadata":{"id":"6JeEqQ1bQ4j3"},"source":["## **Problem 4 (Test Function)**\n","---\n","Here, you will write the code for the testing of the models that you trained in the previous code blocks. \n","\n","The class below provides method to test a given model. It takes a dictionary with the following parameters:\n","\n","1.   device: The device to run the model on.\n","2.   test_data: The test_data dataframe.\n","3.   batch_size: The batch_size which is input to the model.\n","4.   save_path: The directory of your saved model.\n","\n","You would need to implement a single test step in the given loop inside the test() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n","\n","Please write your code between the given TODO block.\n","\n","Hint: This problem is very similar to 3(c)."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"URDu5ew3q5ke"},"outputs":[],"source":["class Tester():\n","\n","  def __init__(self, options):\n","    self.save_path = options['save_path']\n","    self.device = options['device']\n","    self.test_data = options['test_data']\n","    self.batch_size = options['batch_size']\n","    transformer = DistillBERT(self.save_path)\n","    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n","    self.model.to(self.device)\n","\n","  def get_performance_metrics(self, preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n","    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n","    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n","    return precision, recall, f1\n","\n","  def test(self, data_loader):\n","    self.model.eval()\n","    total_recall = 0\n","    total_precision = 0\n","    total_f1 = 0\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","      for (reviews, labels) in tqdm(data_loader):\n","        # TODO(students): start\n","        logits = self.model(reviews.squeeze(1)).logits\n","        loss = torch.nn.CrossEntropyLoss()(logits.to('cpu'), labels.to('cpu'))\n","        \n","        total_loss += loss\n","        precision, recall, f1 = self.get_performance_metrics(preds=logits.detach().cpu().numpy(), labels=labels.detach().cpu().numpy())\n","\n","        total_recall += recall\n","        total_precision += precision\n","        total_f1 += f1\n","\n","        # TODO(students): end\n","    \n","    precision = total_precision/len(data_loader)\n","    recall = total_recall/len(data_loader)\n","    f1 = total_f1/len(data_loader)\n","    loss = total_loss/len(data_loader)\n","\n","    return precision, recall, f1, loss\n","\n","  def execute(self):\n","    test_dataset = DatasetLoader(self.test_data, self.tokenizer)\n","    test_data_loader = test_dataset.get_data_loaders(self.batch_size)\n","\n","    test_precision, test_recall, test_f1, test_loss = self.test(test_data_loader)\n","\n","    print()\n","    print(f'test_loss: {test_loss:.4f} test_precision: {test_precision:.4f} test_recall: {test_recall:.4f} test_f1: {test_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"h1d4bih3SMTM"},"source":["**Notes: Run these blocks only after Experiment 1 to 4 are completed and the models are saved in the \"models\" folder. Copy the output blocks into another text file for report writing.**"]},{"cell_type":"markdown","metadata":{"id":"lkeN4URqRkRD"},"source":["#### **Experiment 5**\n","---\n","Testing your DistilBERT trained with frozen embeddings.\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"uiCG8IQl0qnb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/600 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 600/600 [00:00<00:00, 1132.25it/s]\n","100%|██████████| 38/38 [00:04<00:00,  9.12it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","test_loss: 0.2823 test_precision: 0.8429 test_recall: 0.9198 test_f1: 0.8713\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_frozen_embeddings'\n","tester = Tester(options)\n","tester.execute()"]},{"cell_type":"markdown","metadata":{"id":"VXy2Yv6sRo2F"},"source":["#### **Experiment 6**\n","---\n","Testing your DistilBERT trained with all layers frozen except the final two layers.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"y13wweAicsEJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/600 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 600/600 [00:00<00:00, 885.90it/s]\n","100%|██████████| 38/38 [00:04<00:00,  8.75it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","test_loss: 0.2785 test_precision: 0.8210 test_recall: 0.9404 test_f1: 0.8673\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_top_2_training'\n","tester = Tester(options)\n","tester.execute()"]},{"cell_type":"markdown","metadata":{"id":"dc1CWEB-R06K"},"source":["#### **Experiment 7**\n","---\n","Testing your DistilBERT trained with all layers frozen except the final four layers.\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MvBdkdX4cvSU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/600 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 600/600 [00:00<00:00, 986.47it/s] \n","100%|██████████| 38/38 [00:04<00:00,  8.55it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","test_loss: 0.2995 test_precision: 0.9178 test_recall: 0.8802 test_f1: 0.8888\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_top_4_training'\n","tester = Tester(options)\n","tester.execute()"]},{"cell_type":"markdown","metadata":{"id":"79g8GwS4R3eB"},"source":["#### **Experiment 8**\n","---\n","Testing your DistilBERT trained with all layers trainable.\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"t0wt64hdcyqR"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing data..\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/600 [00:00<?, ?it/s]/home/jwaibong/miniconda3/envs/cse354/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 600/600 [00:00<00:00, 1006.56it/s]\n","100%|██████████| 38/38 [00:04<00:00,  8.53it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","test_loss: 0.3401 test_precision: 0.8593 test_recall: 0.9093 test_f1: 0.8741\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["options = {}\n","options['batch_size'] = BATCH_SIZE\n","options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","options['test_data'] = test_data\n","options['save_path'] = SAVE_PATH + '_all_training'\n","tester = Tester(options)\n","tester.execute()"]},{"cell_type":"markdown","metadata":{"id":"AEPJSuFSz17f"},"source":["## **Results**\n","---\n","\n","Answer the following questions based on the analyses you have performed above:"]},{"cell_type":"markdown","metadata":{"id":"0W-YZW0Y0P2T"},"source":["### 1. Briefly explain your code implementations for each TO-DO task."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F2a8Tg4J0UpW"},"source":["TODO [STUDENT]\n","\n","Problem 1.\n","I used the imported classes AutoModelForSequenceClassification and AutoTokenizer from the transformers library in order to load the pretrained DistilledBert model, making sure to pass num_labels=num_classes in order to set the output dimensions. \n","\n","Problem 2.\n","\n","I used the tokenizer.encode_plus method to encode the input review text into a padded and/or truncated pytorch cuda tensor with shape (1,512). The loop does this for each review in the dataset, making sure to mark the corresponding correct labels in another pytorch long tensor.\n","\n","encoding = self.tokenizer.encode_plus(review, max_length=512, truncation=True, pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')['input_ids'].to('cuda')\n","\n","Problem 3a.\n","\n","When iterating over self.model.named_parameters(), the first element of the tuple is the name which gives information about the corresponding layer (e.g. You would see 'layer.0' in the name for layer 0 as a substring). I used a bunch of if-elif statements to hard code which layers to turn .require_grad on.\n","\n","Problem 3b 3c 4.\n","\n","All of these problems are very similar, except 3c and 4 do not require the model to compute the gradients and backpropogate (via loss.backward and optimizer.step). \n","\n","In order to input the reviews to the model in order to output the logits, I had to squeeze the reviews tensor along dimension 1 because it was a (batch_size, 1, 512) shape tensor and needed to be of shape (batch_size, 512). \n","\n","I used the provided self.get_performance_metrics in order to calculate the current batch's precision, recall, and f1 score (making sure to pass the logits and labels as numpy arrays). Finally, I added the current scores to the total scores.\n","\n","logits = self.model(reviews.squeeze(1)).logits\n","loss = torch.nn.CrossEntropyLoss()(logits.to('cpu'), labels.to('cpu'))\n","\n","loss.backward()\n","optimizer.step()\n","total_loss += loss\n","precision, recall, f1 = self.get_performance_metrics(preds=logits.detach().cpu().numpy(), labels=labels.detach().cpu().numpy())\n","\n","total_recall += recall\n","total_precision += precision\n","total_f1 += f1\n"]},{"cell_type":"markdown","metadata":{"id":"OLgIfurZ0XdZ"},"source":["### 2. A table containing the precision, recall and F1 scores of each DistilBERT model during validation and testing."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["|   Experiment   | Precision | Recall | F1 |\n","| -------------- | -------- | -------- | -------- | \n","| 1 (Validation) | 0.8776 | 0.9302 | 0.8971 |\n","| 2 (Validation) | 0.9119 | 0.9288 | 0.9183 |\n","| 3 (Validation) | 0.9160 | 0.8934 | 0.8997 |\n","| 4 (Validation) | 0.9033 | 0.9655 | 0.9270 |\n","| 5 (Test)       | 0.8429 | 0.9198 | 0.8713 |\n","| 6 (Test)       | 0.8210 | 0.9404 | 0.8673 |\n","| 7 (Test)       | 0.9178 | 0.8802 | 0.8888 |\n","| 8 (Test)       | 0.8593 | 0.9093 | 0.8741 |"]},{"cell_type":"markdown","metadata":{"id":"AlhzgwKx0c1r"},"source":["TODO [STUDENT]"]},{"cell_type":"markdown","metadata":{"id":"j-pmDmAJ0f3I"},"source":["### 3. An analysis explaining your understanding of the impact freezing/training different layers has on the model's performance."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vIEL3iVt0hsl"},"source":["\n","The first observation I had was that freezing all embeddings (meaning keeping the pretrained model as is) still had good performance on this sentiment task. In terms of the metrics, freezing all embeddings was about 3-5 percentage points lower than the highest score in the corresponding dataset (validation or test). This is expected -- the Professor mentioned in lecture that these pretrained language models become pretty good at these tasks even without finetuning because the MLM task helps the model \"understand\" the language.\n","\n","However, finetuning to some degree seems to provide benefit in the metric scores. From what I found, Experiments 3 and 7 (where only the top 4 layers were trainable) provided the most improvement. In this case, these experiments resulted in the highest precision score for both the validation and test set. \n","\n","Overall, the data seems to imply that training ALL the layers does not seem worthwhile. The benefit provided compared to only training some of the layers is marginal whereas the amount of resources/time might be costly in comparison.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p5UR_s7ySnb0"},"source":["## **Submission guidelines**\n","---\n","You would need to submit the following files:\n","\n","\n","1.   `NLP_HW3.ipynb` - This jupyter notebook. It will also work as your report, so please add description to code wherever required. Also make sure to write your analyses outcomes in the RESULTS section above.\n","2.   `gdrive_link.txt` - Should contain a wgetable to a folder that contains your four DistilBERT models. Please make sure you provide the necessary permissions.\n","\n","**Colab design credit**: TA Dhruv Verma"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
